{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../data\")\n",
    "train_path = data_dir / \"train.csv\"\n",
    "\n",
    "df = load_dataset(\"csv\", data_files=train_path.as_posix())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "    You are the most genius human in the world. You know everything especially about science. Now, I will provide you a question with 5 options. Please choose the correct answer. The response you provide must be one among [A, B, C, D, E].\n",
    "    \n",
    "    Question:\n",
    "    {prompt}\\n\n",
    "    \n",
    "    Options:\n",
    "    A) {a}\\n\n",
    "    B) {b}\\n\n",
    "    C) {c}\\n\n",
    "    D) {d}\\n\n",
    "    E) {e}\\n\n",
    "    \n",
    "    Answer: {answer}\n",
    "    '''\n",
    "    \n",
    "prompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b', 'c', 'd', 'e', 'answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    You are the most genius human in the world. You know everything especially about science. Now, I will provide you a question with 5 options. Please choose the correct answer. The response you provide must be one among [A, B, C, D, E].\n",
       "    \n",
       "    Question:\n",
       "    Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
       "\n",
       "    \n",
       "    Options:\n",
       "    A) MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
       "\n",
       "    B) MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
       "\n",
       "    C) MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
       "\n",
       "    D) MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
       "\n",
       "    E) MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
       "\n",
       "    \n",
       "    Answer: D\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = df['train'][0]\n",
    "display(Markdown(prompt.format(prompt=sample['prompt'], \n",
    "                               a=sample['A'], \n",
    "                               b=sample['B'], \n",
    "                               c=sample['C'], \n",
    "                               d=sample['D'], \n",
    "                               e=sample['E'], \n",
    "                               answer=sample['answer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(row):\n",
    "    text = prompt.format(prompt=row['prompt'],\n",
    "                         a=row['A'],\n",
    "                         b=row['B'],\n",
    "                         c=row['C'],\n",
    "                         d=row['D'],\n",
    "                         e=row['E'],\n",
    "                         answer=row['answer'],)\n",
    "    return {\"text\": text}          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e1f05895084a90bbfece3b461524bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'text'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.map(format_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select only indices shorter than 2048\n",
    "def plot_sequence_lengths(data, split='train', max_length=2048):\n",
    "    sequence_lengths = []\n",
    "    keep_indices = []\n",
    "\n",
    "    # Loop over the dataset and get the lengths of text sequences\n",
    "    for i, example in enumerate(data[split]):\n",
    "        sequence_lengths.append(len(example['text']))\n",
    "        if sequence_lengths[i] < max_length:\n",
    "            keep_indices.append(i)\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(sequence_lengths, bins=30)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Text Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    return keep_indices\n",
    "keep_indices_train = plot_sequence_lengths(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index 199 out of range for dataset of size 186.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mselect(keep_indices_train)\n\u001b[0;32m      2\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\arrow_dataset.py:3812\u001b[0m, in \u001b[0;36mDataset.select\u001b[1;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[0;32m   3809\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_contiguous(start, length, new_fingerprint\u001b[39m=\u001b[39mnew_fingerprint)\n\u001b[0;32m   3811\u001b[0m \u001b[39m# If not contiguous, we need to create a new indices mapping\u001b[39;00m\n\u001b[1;32m-> 3812\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_with_indices_mapping(\n\u001b[0;32m   3813\u001b[0m     indices,\n\u001b[0;32m   3814\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   3815\u001b[0m     indices_cache_file_name\u001b[39m=\u001b[39;49mindices_cache_file_name,\n\u001b[0;32m   3816\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   3817\u001b[0m     new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   3818\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\arrow_dataset.py:3943\u001b[0m, in \u001b[0;36mDataset._select_with_indices_mapping\u001b[1;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[0;32m   3941\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m   3942\u001b[0m \u001b[39mif\u001b[39;00m indices:\n\u001b[1;32m-> 3943\u001b[0m     _check_valid_indices_value(\u001b[39mint\u001b[39;49m(\u001b[39mmax\u001b[39;49m(indices)), size\u001b[39m=\u001b[39;49msize)\n\u001b[0;32m   3944\u001b[0m     _check_valid_indices_value(\u001b[39mint\u001b[39m(\u001b[39mmin\u001b[39m(indices)), size\u001b[39m=\u001b[39msize)\n\u001b[0;32m   3945\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\datasets\\arrow_dataset.py:649\u001b[0m, in \u001b[0;36m_check_valid_indices_value\u001b[1;34m(index, size)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_indices_value\u001b[39m(index, size):\n\u001b[0;32m    648\u001b[0m     \u001b[39mif\u001b[39;00m (index \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m index \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[1;32m--> 649\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m out of range for dataset of size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Index 199 out of range for dataset of size 186."
     ]
    }
   ],
   "source": [
    "df['train'] = df['train'].select(keep_indices_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'text'],\n",
       "        num_rows: 186\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227f713c58384631877ce4b2d6ed2f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\warm\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f933c35410f46f580f1951985767cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bed50fec7145518a89756d7d33eeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m base_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      2\u001b[0m     model_id,\n\u001b[0;32m      3\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mbnb_config,\n\u001b[0;32m      4\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      5\u001b[0m     revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m# Using this version because running the new version gives error \u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:488\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    489\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\modeling_utils.py:2297\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2295\u001b[0m     device_map \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mcurrent_device()}\n\u001b[0;32m   2296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2297\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2298\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   2299\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe device_map was not initialized.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2300\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSetting device_map to \u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:torch.cuda.current_device()}.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2301\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIf you want to use the model for inference, please set device_map =\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2302\u001b[0m )\n\u001b[0;32m   2303\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    revision=\"2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5\" # Using this version because running the new version gives error \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f5d0e457034713a79553560bb88cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Failed to load the pretrained model. Error: Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     15\u001b[0m         llm,\n\u001b[0;32m     16\u001b[0m         device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     17\u001b[0m         quantization_config\u001b[39m=\u001b[39;49mquantization_config,\n\u001b[0;32m     18\u001b[0m         max_memory\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m8GB\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     20\u001b[0m     )\u001b[39m.\u001b[39meval()\n\u001b[0;32m     22\u001b[0m     model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m GenerationConfig\u001b[39m.\u001b[39mfrom_pretrained(llm, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    495\u001b[0m     )\n\u001b[0;32m    496\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    497\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\modeling_utils.py:2903\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2894\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   2896\u001b[0m     (\n\u001b[0;32m   2897\u001b[0m         model,\n\u001b[0;32m   2898\u001b[0m         missing_keys,\n\u001b[0;32m   2899\u001b[0m         unexpected_keys,\n\u001b[0;32m   2900\u001b[0m         mismatched_keys,\n\u001b[0;32m   2901\u001b[0m         offload_index,\n\u001b[0;32m   2902\u001b[0m         error_msgs,\n\u001b[1;32m-> 2903\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   2904\u001b[0m         model,\n\u001b[0;32m   2905\u001b[0m         state_dict,\n\u001b[0;32m   2906\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   2907\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2908\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2909\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   2910\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   2911\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   2912\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   2913\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   2914\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   2915\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   2916\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   2917\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[0;32m   2918\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   2919\u001b[0m     )\n\u001b[0;32m   2921\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\modeling_utils.py:3260\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3259\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m-> 3260\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[0;32m   3261\u001b[0m         model_to_load,\n\u001b[0;32m   3262\u001b[0m         state_dict,\n\u001b[0;32m   3263\u001b[0m         loaded_keys,\n\u001b[0;32m   3264\u001b[0m         start_prefix,\n\u001b[0;32m   3265\u001b[0m         expected_keys,\n\u001b[0;32m   3266\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3267\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3268\u001b[0m         offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[0;32m   3269\u001b[0m         state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[0;32m   3270\u001b[0m         state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[0;32m   3271\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3272\u001b[0m         is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[0;32m   3273\u001b[0m         is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[0;32m   3274\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3275\u001b[0m     )\n\u001b[0;32m   3276\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\modeling_utils.py:725\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[1;32m--> 725\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[0;32m    726\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[0;32m    727\u001b[0m             )\n\u001b[0;32m    729\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\transformers\\utils\\bitsandbytes.py:109\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 109\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\warm\\.conda\\envs\\llm_science39\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m GenerationConfig\u001b[39m.\u001b[39mfrom_pretrained(llm, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFailed to load the pretrained model. Error: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "\u001b[1;31mException\u001b[0m: Failed to load the pretrained model. Error: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# quantization configuration for NF4 (4 bits)about:blank#blocked\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm,\n",
    "        device_map=\"cuda:0\",\n",
    "        quantization_config=quantization_config,\n",
    "        max_memory=\"8GB\",\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "\n",
    "    model.generation_config = GenerationConfig.from_pretrained(llm, trust_remote_code=True)\n",
    "except Exception as e:\n",
    "    raise Exception(\"Failed to load the pretrained model. Error: \" + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
